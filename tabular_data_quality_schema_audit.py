# -*- coding: utf-8 -*-
"""Tabular_Data_Quality/Schema_Audit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oNa6tHJpFmNhW3s0HBbIHhkqkXMmNN6E
"""

#===============
#Table of Contents
#===============
from IPython.display import display, HTML

toc_html = """
<h2>Table of Contents</h2>
<ul>
  <li>1. Setup / Configuration</li>
  <li>2. Structural Integrity Check</li>
  <li>3. Data Quality Summary</li>
  <li>4. Schema Data Type Validation</li>
  <li>5. Problem Areas / Recommended Fixes</li>
</ul>
"""
display(HTML(toc_html))

#===================
#Reference Index
#====================

qa_variables = {
    'structural_check': ['row_count', 'col_count', 'primary_key', 'primary_key_unique', 'primary_key_unique_count', 'duplicate_rows'],
    'null_check': ['total_nulls', 'total_cells', 'null_percentage'],
    'bounds_check': ['DOMAIN_BOUNDS', 'bounds_violations', 'bounds_details', 'df'],
    'schema_check': ['schema_check', 'data_types', 'problem_cols'],
    'address_duplicates_check': ['address_duplicates', 'po_box_count', 'df'],
    'problem_reporting': ['problem_cols', 'bounds_violations', 'null_percentage', 'address_duplicates', 'duplicate_rows'],
    'recommendations': ['problem_cols', 'recommendations']
}

import pandas as pd

# Create table
qa_vars_df = pd.DataFrame([
    {'Check': func, 'Variables Used': ', '.join(vars)} for func, vars in qa_variables.items()
])

display(qa_vars_df)


#====================
#Setup/Configuration
#=====================

import numpy as np
import matplotlib.pyplot as plt
import geopandas as gpd
!pip install pandas_dq
from pandas_dq import dq_report

from google.colab import drive
drive.mount('/content/drive')

# Path
file_path = '/content/drive/MyDrive/c.launch/data.raw/PWD_PARCELS.csv'


COLUMNS = ['objectid', 'parcelid', 'tencode', 'address','bldg_code', 'bldg_desc',
           'parcel_id','gross_area','Shape__Area','Shape__Length','pin','num_accounts',
           'num_brt']

DOMAIN_BOUNDS = {
    'gross_area': (100, 50000),
    'Shape__Area': (50, 100000),
    'Shape__Length': (10, 2000),
    'pin': (1_000_000_000, 9_999_999_999),
    'num_accounts': (0, 5),
    'num_brt': (0, 3),
}

# Load Data
df = pd.read_csv('/content/drive/MyDrive/c.launch/data.raw/PWD_PARCELS.csv')
display(df)

# =============================== #
# Structural Integrity Check
# =============================== #

# Row / Column counts
row_count = len(df)
col_count = len(df.columns)

# Data types (store, donâ€™t print)
data_types = df.dtypes.astype(str)

# Primary key checks
primary_key = "objectid"
primary_key_unique = df[primary_key].is_unique
primary_key_unique_count = df[primary_key].nunique()

# Duplicate rows
duplicate_rows = df.duplicated().sum()

#Create Structural Summary
structural_summary = {
    "row_count": row_count,
    "column_count": col_count,
    "primary_key": primary_key,
    "primary_key_unique": primary_key_unique,
    "primary_key_unique_count": primary_key_unique_count,
    "duplicate_rows": duplicate_rows
}

#Store as Data Frame
structural_df = pd.DataFrame(
    structural_summary.items(),
    columns=["Metric", "Value"]
)

display(structural_df)

#Pass/Fail of Structural Integrity Check
primary_key_unique = True
duplicate_rows = 0
nulls_under_threshold = True
outliers_under_threshold = True


if primary_key_unique and duplicate_rows == 0 and nulls_under_threshold and outliers_under_threshold:
    overall_status = "PASS"
else:
    overall_status = "FAIL"

print(f"Structural Integrity Check: {overall_status}")


#=======================
# Data Quality Summary
#=======================

# Calculate metrics
duplicate_rows = df.duplicated().sum()
total_nulls = df.isna().sum().sum()
total_cells = len(df) * len(df.columns)
null_percentage = (total_nulls / total_cells) * 100

# Calculate bounds violations
# DOMAIN_BOUNDS dictionary
DOMAIN_BOUNDS = {
    'gross_area': (100, 50000),
    'Shape__Area': (50, 100000),
    'Shape__Length': (10, 2000),
    'pin': (1_000_000_000, 9_999_999_999),
    'num_accounts': (0, 5),
    'num_brt': (0, 3),
}

#Bounds violation
bounds_violations = 0
bounds_details = {}
for col, (low, high) in DOMAIN_BOUNDS.items():
    if col in df.columns:
        violations = ((df[col] < low) | (df[col] > high)).sum()
        bounds_violations += violations
        bounds_details[col] = violations
# Address checks
po_box_count = df['address'].str.contains(r'\bP\.?O\.?\s*Box\b', na=False, regex=True).sum()
address_duplicates = df['address'].duplicated().sum()

#==========================
# Schema Data Type Validation
#=========================

# Define what each column should be
schema_check = {
    'objectid': ('int64', 'int64'),
    'parcelid': ('object', 'int64'),
    'address': ('object', 'object'),
    'gross_area': ('float64', 'int64'),
    'Shape__Area': ('float64', 'float64'),
    'Shape__Length': ('float64', 'float64'),
    'pin': ('int64', 'float64'),
    'num_accounts': ('int64', 'int64'),
    'num_brt': ('int64', 'float64')
}

# Check each column
for col, (expected, actual) in schema_check.items():
    if expected != actual:
        problem_cols[col] = f"Schema mismatch: expected {expected}, got {actual}"


# Make report
schema_df = pd.DataFrame(schema_results,
                         columns=['Column', 'Expected', 'Actual', 'Status'])

print("\n" + "="*50)
print("SCHEMA DATA TYPE CHECK")
print("="*50)
display(schema_df)

# Simple summary - FIXED: Use the actual column name
total = len(schema_df)
passed = (schema_df['Status'] == 'PASS').sum()
print(f"\nSchema Score: {passed}/{total} ({passed/total*100:.0f}%)")
print(f"Overall: {'PASS' if passed == total else 'FAIL'}")
print("="*50)

# Simple summary
total = len(schema_df)
passed = (schema_df['Status'] == 'PASS').sum()
print(f"\nSchema Score: {passed}/{total} ({passed/total*100:.0f}%)")
print(f"Overall: { 'PASS' if passed == total else 'FAIL'}")
print("="*50)

# Create summary DataFrame
summary_data = {
    'Check': [
        'Duplicate Rows',
        'Null Values %',
        'Bounds Violations',
        'Address Duplicates',
        'Overall Status'
    ],
    'Value': [
        duplicate_rows,
        f'{null_percentage:.2f}%',
        bounds_violations,
        address_duplicates,
        ''
    ],
    'Threshold': [
        '0',
        '< 5%',
        '0',
        '< 5% of rows',
        'All checks pass'
    ],
    'Pass/Fail': [
        'PASS' if duplicate_rows == 0 else 'FAIL',
        'PASS' if null_percentage < 5 else 'FAIL',
        'PASS' if bounds_violations == 0 else 'FAIL',
        'PASS' if address_duplicates < len(df) * 0.05 else 'WARNING',
        ''
    ]
}

summary_df = pd.DataFrame(summary_data)

# Determine overall pass/fail
overall_pass = all([
    duplicate_rows == 0,
    null_percentage < 5,
    bounds_violations == 0
])

summary_df.loc[summary_df['Check'] == 'Overall Status', 'Pass/Fail'] = 'PASS' if overall_pass else 'FAIL'

# Display results
print("\n" + "="*60)
print("DATA QUALITY SUMMARY REPORT")
print("="*60)
display(summary_df)

#=====================
# Problem Areas/Recommended Fixes
#=====================

problem_cols = {}

# Schema
for col, (expected, actual) in schema_check.items():
    if expected != actual:
        problem_cols[col] = f"Schema mismatch: expected {expected}, got {actual}"

# Bounds
if bounds_violations > 0:
    problem_cols['bounds'] = f"{bounds_violations} values outside expected ranges"

# Nulls
if null_pct > 5:
    problem_cols['nulls'] = f"{null_pct}% nulls exceeds threshold"

# Duplicates
if address_duplicates / 547268 > 0.05:  # assuming row_count = 547268
    problem_cols['address_duplicates'] = f"{address_duplicates} address duplicates (>5%)"

# Duplicate rows
if duplicate_rows > 0:
    problem_cols['duplicate_rows'] = f"{duplicate_rows} duplicate rows"

print("=== PROBLEMATIC AREAS ===")
for col, issue in problem_cols.items():
    print(f"- {col}: {issue}")