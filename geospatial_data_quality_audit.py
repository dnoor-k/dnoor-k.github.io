# -*- coding: utf-8 -*-
"""Geospatial_Data_Quality_Audit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RdRd8FMCk6DlvyqbfiPjQDEzTrNWjRjU

================================================================================
PHILADELPHIA STORMWATER BILLING PARCEL QA PIPELINE - DOCUMENTATION
================================================================================

TABLE OF CONTENTS
--------------------------------------------------------------------------------
# | Section
--------------------------------------------------------------------------------
1 | Setup/Configuration

2 | Reprojection/Visualization

3 | Feature Check, Recalculation, Data Type Check

4 | Geometry Validation and Logging

5 | Polygon Topology Validation

6 | Positional Accuracy Check

7 | Final QA Summary/Recommendations

8 | Geometric Visualization of Errors

###FUNCTION LOG

| Function | Returns | Purpose |
|----------|---------|---------|
| `check_geometries(gdf)` | DataFrame | Check for null, empty, invalid geometries |
| `check_polygon_topology(gdf, sample_size, sample_every)` | DataFrame | Check overlaps, slivers, intersections |
| `generate_qa_summary(gdf, geo_report, topo_report, pos_check)` | Tuple | Generate QA summary with severity flags |
| `visualize_issues(gdf, geometry_report, topology_report)` | Folium Map | Create interactive map of issues |

---
###VARIABLE LOG

| Variable | Type | Description |
|----------|------|-------------|
| `COLUMNS` | list | 17 expected column names |
| `DOMAIN_BOUNDS` | dict | Value ranges for fields |
| `EXPECTED_TYPES` | dict | Expected data types |
| `PHILLY_BOUNDS` | dict | Geographic bounding box |
| `file_path` | str | Path to GeoJSON file |
| `gdf` | GeoDataFrame | Original data (4326) |
| `gdf_metric` | GeoDataFrame | Reprojected data (32618) |
| `projection_info` | dict | CRS parameters |
| `geometry_report` | DataFrame | Geometry validation results |
| `topology_report` | DataFrame | Topology check results |
| `positional_check` | dict | Positional accuracy metrics |
| `outside_bounds` | GeoDataFrame | Parcels outside bounds |
| `area_outliers` | GeoDataFrame | Statistical area outliers |
| `spatial_outliers` | GeoDataFrame | Parcels far from center |
| `qa_summary` | DataFrame | Final QA metrics |
| `qa_flags` | DataFrame | Severity flags |
| `qa_status` | str | Overall QA status |
| `qa_map` | folium.Map | Interactive visualization |       


*NOTE: ACCESSING MAP MAY OR MAY NOT REQUIRE SEPERATELY LOADING SAVED HTML (`qa_map.html`)
"""

#====================
#Setup/Configuration
#=====================

import geopandas as gpd
import pandas as pd

!pip install 'pandera[pandas]'
import pandera as pa
from shapely import wkt
import matplotlib.pyplot as plt
import folium
from shapely.validation import explain_validity
import numpy as np
import warnings

from google.colab import drive
drive.mount('/content/drive')

#Defining Data Contents
COLUMNS = ['objectid', #Record ID
           'parcelid', # Parcel Description
           'tencode', #ten digit code
           'address',
           'owner1',
           'owner2',
           'bldg_code',#Building Classification/Type
           'bldg_desc', #Building Description
           'brt_id', #Property Tax ID
           'num_brt', #BRT accounts per parcel
           'num_accounts', #Accounts prt property record
           'gross_area', #Total Square Feet
           'pin', #ID number
           'parcel_id',
           'Shape__Area', #Sq Feet Area
           'Shape__Length', #Sq Feet Perimeter
           'geometry'] #spatial data

#Reasonable Bounds
DOMAIN_BOUNDS = {
    'gross_area': (100, 50000),          # sq ft
    'Shape__Area': (50, 100000),         # sq units
    'Shape__Length': (10, 2000),         # linear units
    'pin': (1_000_000_000, 9_999_999_999),  # 10-digit PIN
    'num_accounts': (0, 5),              # Accounts per parcel
    'num_brt': (0, 3),                   # BRT accounts per parcel
}

#Expected Data Types per Column

EXPECTED_TYPES = {
    'objectid': 'int64',
    'parcelid': 'object',
    'tencode': 'object',
    'address': 'object',
    'owner1': 'object',
    'owner2': 'object',
    'bldg_code': 'object',
    'bldg_desc': 'object',
    'brt_id': 'object',
    'num_brt': 'int64',
    'num_accounts': 'int64',
    'gross_area': 'float64',
    'pin': 'int64',
    'parcel_id': 'object',
    'Shape__Area': 'float64',
    'Shape__Length': 'float64',
    'geometry': 'geometry'
}
#Data Path
file_path = '/content/drive/MyDrive/c.launch/PWD_PARCELS.geojson'

#Read GeoJSON
gdf = gpd.read_file('/content/drive/MyDrive/c.launch/PWD_PARCELS.geojson')
display(gdf)


#=======================
#Reprojection/Visualization
#=======================

#Create projction data dictionary
projection_info = {
    'original_crs': str(gdf.crs),
    'target_crs': 'EPSG:32618',  # UTM Zone 18N for Philadelphia
    'units': 'meters'}


#Reprojection - Make Proj Local
gdf_metric = gdf.to_crs(projection_info['target_crs'])

#============================================
#Feature Check, Recalculation, Data Type Check
#============================================

#Feature Count/Geometry Check
geometry_check = {}

# COUNTS
geometry_check['total_features'] = len(gdf_metric)
geometry_check['total_columns'] = len(gdf_metric.columns)


#Recalculate Geometric Attributes
gdf_metric['Shape__Area_recalc_m2'] = gdf_metric.geometry.area
gdf_metric['Shape__Length_recalc_m'] = gdf_metric.geometry.length

gdf_metric['area_m2'] = gdf_metric.geometry.area
gdf_metric['perimeter_m'] = gdf_metric.geometry.length

#Feature Check/Geometry Count
feature_check = {
    'total_features': len(gdf_metric),
    'total_columns': len(gdf_metric.columns),
    'null_geometries': gdf_metric.geometry.isna().sum(),
    'geometry_type': gdf_metric.geometry.geom_type.mode()[0]  # Most common type
}

# Data type validation
dtype_issues = []
for col, expected in EXPECTED_TYPES.items():
    if col in gdf_metric.columns:
        actual = str(gdf_metric[col].dtype)
        if actual != expected:
            dtype_issues.append({'column': col, 'expected': expected, 'actual': actual})

summary_data = {
    **projection_info,
    **feature_check,
    'dtype_mismatches': len(dtype_issues)
}

summary_df = pd.DataFrame([summary_data]).T
summary_df.columns = ['Value']
summary_df.index.name = 'Metric'

print("PROJECTION & GEOMETRY SUMMARY:")
display(summary_df)


#===========================
#Visualization -- 3 Plots/Side by Side Comparisons

## Plots for Shape_Area, Shape_Length, and gross_area Columns
#===========================

fig, axes = plt.subplots(1, 3, figsize=(22, 7))

# Plot 1 - Shape__Area
gdf_metric.plot(column='Shape__Area', ax=axes[0], legend=True, cmap='YlOrRd',
              edgecolor='black', linewidth=0.2)
axes[0].set_title('Shape__Area (sq ft)', fontsize=14, fontweight='bold')

# Plot 2 - Shape__Length
gdf_metric.plot(column='Shape__Length', ax=axes[1], legend=True, cmap='viridis',
              edgecolor='black', linewidth=0.2)
axes[1].set_title('Shape__Length (ft)', fontsize=14, fontweight='bold')

# Plot 3 - gross_area
gdf_metric.plot(column='gross_area', ax=axes[2], legend=True, cmap='plasma',
              edgecolor='black', linewidth=0.2)
axes[2].set_title('gross_area (sq ft)', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

#===============================
# Geometry Validation and Logging
#===============================


def check_geometries(gdf):
    """Check geometry issues, return report dataframe"""

    issues = []

    for idx, row in gdf.iterrows():
        geom = row['geometry']
        issue_type = None

        # Check null
        if geom is None:
            issue_type = 'null'
        # Check empty
        elif geom.is_empty:
            issue_type = 'empty'
        # Check invalid
        elif not geom.is_valid:
            issue_type = f'invalid: {explain_validity(geom)}'

        if issue_type:
            issues.append({
                'index': idx,
                'objectid': row.get('objectid'),
                'parcelid': row.get('parcelid'),
                'issue': issue_type
            })

    return pd.DataFrame(issues)


# Check geometries
geometry_report = check_geometries(gdf)

print(f"\nTotal records: {len(gdf)}")
print(f"Issues found: {len(geometry_report)}")

if len(geometry_report) > 0:
    display(geometry_report)
else:
    print("No geometry issues found!")



#==========================
#Polygon Topology Validation
##Check for Overlaps, Slivers, and Intersections
###sample every 60th polygon/1k polygons (dataset has 547k) to save RAM
#==========================

def check_geometries(gdf):
    """Check geometry issues, return report dataframe"""

    issues = []

    for idx, row in gdf.iterrows():
        geom = row['geometry']
        issue_type = None

        # Check null
        if geom is None:
            issue_type = 'null'
        # Check empty
        elif geom.is_empty:
            issue_type = 'empty'
        # Check invalid
        elif not geom.is_valid:
            issue_type = f'invalid: {explain_validity(geom)}'

        if issue_type:
            issues.append({
                'index': idx,
                'objectid': row.get('objectid'),
                'parcelid': row.get('parcelid'),
                'issue': issue_type
            })

    return pd.DataFrame(issues)

# Check geometries
geometry_report = check_geometries(gdf)
print(f"\nTotal records: {len(gdf)}")
print(f"Issues found: {len(geometry_report)}")
if len(geometry_report) > 0:
    display(geometry_report)
else:
    print("No geometry issues found!")

#==========================
# Polygon Topology Validation
# Check for Overlaps, Slivers, and Intersections
#==========================

def check_polygon_topology(gdf, sample_interval=30, max_parcels=5000):
    """Check for overlaps, slivers, and intersections between polygons"""
    topology_issues = []

    # Filter valid geometries
    valid_gdf = gdf[gdf['geometry'].notna() & ~gdf['geometry'].is_empty].copy()

    # Sample every Nth parcel
    sampled_gdf = valid_gdf.iloc[::sample_interval].copy()

    # Limit to max_parcels
    if len(sampled_gdf) > max_parcels:
        sampled_gdf = sampled_gdf.iloc[:max_parcels]

    print(f"Checking topology for {len(sampled_gdf)} sampled polygons (every {sample_interval}th parcel)")

    # Check topology issues
    for i, row_i in sampled_gdf.iterrows():
        geom_i = row_i['geometry']
        if not geom_i.is_valid:
            continue

        for j, row_j in sampled_gdf.iterrows():
            if i >= j:
                continue

            geom_j = row_j['geometry']
            if not geom_j.is_valid:
                continue

            # Check overlap
            if geom_i.overlaps(geom_j):
                topology_issues.append({
                    'issue_type': 'overlap',
                    'parcel_1': row_i.get('parcelid'),
                    'parcel_2': row_j.get('parcelid'),
                    'index_1': i,
                    'index_2': j,
                    'area': geom_i.intersection(geom_j).area
                })

            # Check sliver
            elif geom_i.touches(geom_j):
                distance = geom_i.distance(geom_j)
                if distance > 0 and distance < 0.1:
                    topology_issues.append({
                        'issue_type': 'sliver',
                        'parcel_1': row_i.get('parcelid'),
                        'parcel_2': row_j.get('parcelid'),
                        'index_1': i,
                        'index_2': j,
                        'distance': distance
                    })

            # Check intersection
            elif geom_i.intersects(geom_j):
                intersection = geom_i.intersection(geom_j)
                if intersection.area > 0:
                    topology_issues.append({
                        'issue_type': 'intersection',
                        'parcel_1': row_i.get('parcelid'),
                        'parcel_2': row_j.get('parcelid'),
                        'index_1': i,
                        'index_2': j,
                        'area': intersection.area
                    })

    return pd.DataFrame(topology_issues)

# Run topology check with 5k parcels, 30th sampling
topology_report = check_polygon_topology(gdf_metric, sample_interval=30, max_parcels=5000)

print(f"\nTopology issues found: {len(topology_report)}")

if len(topology_report) > 0:
    print(f"\nIssue breakdown:")
    print(topology_report['issue_type'].value_counts())
    display(topology_report.head(20))
else:
    print("No topology issues detected!")

#===============================
#Positional Accuracy Check
#===============================

positional_check = {}

# 1. BOUNDS VALIDATION
PHILLY_BOUNDS = {
    'min_x': -75.28,
    'max_x': -74.96,
    'min_y': 39.87,
    'max_y': 40.14
}

#Wrap Bounds Check
with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    gdf_wgs84 = gdf_metric.to_crs('EPSG:4326')
    outside_bounds = gdf_wgs84[
        (gdf_wgs84.geometry.centroid.x < PHILLY_BOUNDS['min_x']) |
        (gdf_wgs84.geometry.centroid.x > PHILLY_BOUNDS['max_x']) |
        (gdf_wgs84.geometry.centroid.y < PHILLY_BOUNDS['min_y']) |
        (gdf_wgs84.geometry.centroid.y > PHILLY_BOUNDS['max_y'])
    ]

    positional_check['bounds_valid'] = len(outside_bounds) == 0
# Get actual bounds
actual_bounds = gdf_metric.total_bounds  # [minx, miny, maxx, maxy]

positional_check['data_min_x'] = actual_bounds[0]
positional_check['data_min_y'] = actual_bounds[1]
positional_check['data_max_x'] = actual_bounds[2]
positional_check['data_max_y'] = actual_bounds[3]

# Check if data extends beyond expected bounds
positional_check['bounds_valid'] = (
    actual_bounds[0] >= PHILLY_BOUNDS['min_x'] and
    actual_bounds[2] <= PHILLY_BOUNDS['max_x'] and
    actual_bounds[1] >= PHILLY_BOUNDS['min_y'] and
    actual_bounds[3] <= PHILLY_BOUNDS['max_y']
)

# Find parcels outside expected bounds (in original CRS for lat/lon check)
gdf_wgs84 = gdf_metric.to_crs('EPSG:4326')  # Convert to lat/lon for bounds check
outside_bounds = gdf_wgs84[
    (gdf_wgs84.geometry.centroid.x < PHILLY_BOUNDS['min_x']) |
    (gdf_wgs84.geometry.centroid.x > PHILLY_BOUNDS['max_x']) |
    (gdf_wgs84.geometry.centroid.y < PHILLY_BOUNDS['min_y']) |
    (gdf_wgs84.geometry.centroid.y > PHILLY_BOUNDS['max_y'])
]

positional_check['parcels_outside_bounds'] = len(outside_bounds)

# 2. OUTLIER DETECTION

# Statistical outliers in area
area_mean = gdf_metric['area_m2'].mean()
area_std = gdf_metric['area_m2'].std()

# Flag parcels > 3 standard deviations from mean
area_outliers = gdf_metric[
    (gdf_metric['area_m2'] > area_mean + 3 * area_std) |
    (gdf_metric['area_m2'] < area_mean - 3 * area_std)
]

positional_check['area_outliers'] = len(area_outliers)
positional_check['area_outliers_pct'] = round(len(area_outliers) / len(gdf_metric) * 100, 2)

# Spatial outliers

# Get centroids
gdf_metric['centroid_x'] = gdf_metric.geometry.centroid.x
gdf_metric['centroid_y'] = gdf_metric.geometry.centroid.y

# Calculate distance from geographic center
center_x = gdf_metric['centroid_x'].median()
center_y = gdf_metric['centroid_y'].median()

gdf_metric['dist_from_center'] = np.sqrt(
    (gdf_metric['centroid_x'] - center_x)**2 +
    (gdf_metric['centroid_y'] - center_y)**2
)

# Flag spatial outliers (> 3 std dev from center)
dist_mean = gdf_metric['dist_from_center'].mean()
dist_std = gdf_metric['dist_from_center'].std()

spatial_outliers = gdf_metric[
    gdf_metric['dist_from_center'] > dist_mean + 3 * dist_std
]

positional_check['spatial_outliers'] = len(spatial_outliers)

# 3. SUMMARY
positional_summary = pd.DataFrame([positional_check]).T
positional_summary.columns = ['Value']
positional_summary.index.name = 'Check'

print("="*60)
print("POSITIONAL ACCURACY CHECKS")
print("="*60)
display(positional_summary)

# 4. FLAG PROBLEMS
if len(outside_bounds) > 0:
    print(f"\n  {len(outside_bounds)} parcels outside expected bounds:")
    display(outside_bounds[['objectid', 'parcelid', 'address']].head(10))

if len(area_outliers) > 0:
    print(f"\n  {len(area_outliers)} area outliers (>3σ from mean):")
    display(area_outliers[['objectid', 'parcelid', 'area_m2']].head(10))

if len(spatial_outliers) > 0:
    print(f"\n  {len(spatial_outliers)} spatial outliers (far from cluster):")
    display(spatial_outliers[['objectid', 'parcelid', 'dist_from_center']].head(10))


#===============================
# Final QA Summary/ Recommendations
#===============================

def generate_qa_summary(gdf, geometry_report, topology_report, positional_check):
    """Generate final QA summary with flags and recommendations"""

    # Calculate key metrics
    total_records = len(gdf)
    geometry_issues = len(geometry_report)
    topology_issues = len(topology_report)

    # Severity flags
    flags = []

    # CRITICAL FLAGS
    if geometry_issues > 0:
        flags.append({
            'severity': 'CRITICAL',
            'category': 'Geometry',
            'issue': f'{geometry_issues} invalid/null/empty geometries found',
            'impact': 'Cannot perform spatial operations on invalid geometries',
            'recommendation': 'Run repair function or exclude invalid records'
        })

    if topology_issues > 0:
        overlap_count = len(topology_report[topology_report['issue_type'] == 'overlap'])
        if overlap_count > 0:
            flags.append({
                'severity': 'HIGH',
                'category': 'Topology',
                'issue': f'{overlap_count} overlapping polygons detected',
                'impact': 'Area calculations will be incorrect, parcels conflict',
                'recommendation': 'Review and resolve overlaps - may indicate data errors'
            })

        sliver_count = len(topology_report[topology_report['issue_type'] == 'sliver'])
        if sliver_count > 0:
            flags.append({
                'severity': 'MEDIUM',
                'category': 'Topology',
                'issue': f'{sliver_count} sliver gaps detected',
                'impact': 'Small gaps between parcels may cause coverage issues',
                'recommendation': 'Consider snapping tolerance or boundary adjustment'
            })

    # POSITIONAL FLAGS
    if positional_check.get('parcels_outside_bounds', 0) > 0:
        flags.append({
            'severity': 'HIGH',
            'category': 'Position',
            'issue': f"{positional_check['parcels_outside_bounds']} parcels outside Philadelphia bounds",
            'impact': 'Data may include incorrect locations or wrong CRS',
            'recommendation': 'Verify CRS and check parcel addresses'
        })

    if positional_check.get('area_outliers', 0) > 0:
        pct = positional_check['area_outliers_pct']
        if pct > 1:  # More than 1% outliers
            flags.append({
                'severity': 'MEDIUM',
                'category': 'Position',
                'issue': f"{positional_check['area_outliers']} area outliers ({pct}%)",
                'impact': 'Unusual parcel sizes may indicate data errors',
                'recommendation': 'Review largest and smallest parcels for validity'
            })

    if positional_check.get('spatial_outliers', 0) > 0:
        flags.append({
            'severity': 'LOW',
            'category': 'Position',
            'issue': f"{positional_check['spatial_outliers']} spatial outliers",
            'impact': 'Some parcels far from city center',
            'recommendation': 'Verify these are legitimate edge parcels'
        })

    # Create flags dataframe
    flags_df = pd.DataFrame(flags)

    # Overall summary
    summary = {
        'total_records': total_records,
        'geometry_issues': geometry_issues,
        'topology_issues_found': topology_issues,
        'topology_sample_size': len(topology_report) if len(topology_report) > 0 else 'N/A',
        'parcels_outside_bounds': positional_check.get('parcels_outside_bounds', 0),
        'area_outliers': positional_check.get('area_outliers', 0),
        'spatial_outliers': positional_check.get('spatial_outliers', 0),
        'critical_flags': len([f for f in flags if f['severity'] == 'CRITICAL']),
        'high_flags': len([f for f in flags if f['severity'] == 'HIGH']),
        'medium_flags': len([f for f in flags if f['severity'] == 'MEDIUM']),
        'low_flags': len([f for f in flags if f['severity'] == 'LOW'])
    }

    summary_df = pd.DataFrame([summary]).T
    summary_df.columns = ['Count']
    summary_df.index.name = 'Metric'

    # Overall status
    if summary['critical_flags'] > 0:
        status = ' CRITICAL ISSUES - Action Required'
    elif summary['high_flags'] > 0:
        status = ' HIGH PRIORITY - Review Recommended'
    elif summary['medium_flags'] > 0:
        status = ' MEDIUM PRIORITY - Monitor'
    else:
        status = ' PASSED - Minor Issues Only'

    return summary_df, flags_df, status


# Generate QA Summary
qa_summary, qa_flags, qa_status = generate_qa_summary(
    gdf_metric,
    geometry_report,
    topology_report,
    positional_check
)

# Display Results
print("="*80)
print("FINAL QA SUMMARY REPORT")
print("="*80)
print(f"\nOVERALL STATUS: {qa_status}\n")

print("SUMMARY METRICS:")
display(qa_summary)

if len(qa_flags) > 0:
    print("\n" + "="*80)
    print("FLAGGED ISSUES & RECOMMENDATIONS")
    print("="*80)
    display(qa_flags)

    # Priority recommendations
    critical = qa_flags[qa_flags['severity'] == 'CRITICAL']
    if len(critical) > 0:
        print("\n CRITICAL ACTIONS REQUIRED:")
        for _, row in critical.iterrows():
            print(f"  • {row['issue']}")
            print(f"    → {row['recommendation']}\n")

    high = qa_flags[qa_flags['severity'] == 'HIGH']
    if len(high) > 0:
        print(" HIGH PRIORITY RECOMMENDATIONS:")
        for _, row in high.iterrows():
            print(f"  • {row['issue']}")
            print(f"    → {row['recommendation']}\n")
else:
    print("\n No critical issues detected!")

#==================================
# Geometric Visualization of Errors
#==================================

import folium

def visualize_issues(gdf, geometry_report, topology_report):
    """Map showing all issues"""

    gdf_map = gdf.to_crs('EPSG:4326')
    center = [gdf_map.geometry.centroid.y.mean(), gdf_map.geometry.centroid.x.mean()]

    m = folium.Map(location=center, zoom_start=11, tiles='CartoDB positron')

    # Invalid geometries - RED
    if len(geometry_report) > 0:
        for idx, row in geometry_report.iterrows():
            if row['index'] in gdf_map.index:
                parcel = gdf_map.loc[row['index']]
                if parcel['geometry'] is not None:
                    folium.GeoJson(
                        parcel['geometry'],
                        style_function=lambda x: {'fillColor': 'red', 'color': 'red', 'weight': 2},
                        tooltip=f"Invalid: {row['parcelid']}"
                    ).add_to(m)

    # Overlaps - ORANGE
    if len(topology_report) > 0:
        for idx, row in topology_report[topology_report['issue_type'] == 'overlap'].iterrows():
            if row['index_1'] in gdf_map.index:
                parcel = gdf_map.loc[row['index_1']]
                folium.GeoJson(
                    parcel['geometry'],
                    style_function=lambda x: {'fillColor': 'orange', 'color': 'orange', 'weight': 2},
                    tooltip=f"Overlap: {row['parcel_1']}"
                ).add_to(m)

    m.save('/content/drive/MyDrive/c.launch/qa_map.html')
    print("✓ Map saved: qa_map.html")
    return m

# Create map
qa_map = visualize_issues(gdf_metric, geometry_report, topology_report)
qa_map